# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mXsXyLlfp7gMpT6siuDT7m4ISbfnK5WN
"""

# ================================
# Import Necessary Libraries
# ================================
import os
import numpy as np
import matplotlib.pyplot as plt
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay

from tensorflow.keras.applications import ResNet50
from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping
from tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization, Input
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.metrics import Precision, Recall

# ================================
# 1. Load and Prepare Dataset
# ================================
DATASET_PATH = '/content/drive/MyDrive/Major_Project/dataset'
OUTPUT_PATH = '/content/drive/MyDrive/Major_Project/Final/diabetic/dataset_subsets'

# Split the dataset into training, validation, and test sets (70%, 10%, 20%)
import splitfolders
if not os.path.exists(OUTPUT_PATH):
    splitfolders.ratio(DATASET_PATH, output=OUTPUT_PATH, seed=123, ratio=(.7, .1, .2))

train_dir = os.path.join(OUTPUT_PATH, 'train')
valid_dir = os.path.join(OUTPUT_PATH, 'val')
test_dir = os.path.join(OUTPUT_PATH, 'test')

# ================================
# 2. Data Augmentation and Preprocessing
# ================================
IMG_DIMS = 224  # Standard size for ResNet50
BATCH_SIZE = 16

# Training data generator with augmentation
train_gen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True
)

train_data = train_gen.flow_from_directory(
    train_dir,
    target_size=(IMG_DIMS, IMG_DIMS),
    batch_size=BATCH_SIZE,
    class_mode='categorical'
)

# Validation and test data generator (no augmentation)
val_gen = ImageDataGenerator(rescale=1./255)
val_data = val_gen.flow_from_directory(
    valid_dir,
    target_size=(IMG_DIMS, IMG_DIMS),
    batch_size=BATCH_SIZE,
    class_mode='categorical'
)

test_gen = ImageDataGenerator(rescale=1./255)
test_data = test_gen.flow_from_directory(
    test_dir,
    target_size=(IMG_DIMS, IMG_DIMS),
    batch_size=BATCH_SIZE,
    class_mode='categorical'
)

# ================================
# 3. Define Hyperparameters
# ================================
EPOCHS = 10
LEARNING_RATE = 0.0001
METRICS = ['categorical_accuracy', Precision(), Recall()]

# ================================
# 4. Build ResNet50 Model
# ================================
# Define input tensor explicitly
input_tensor = Input(shape=(IMG_DIMS, IMG_DIMS, 3))

# Load pretrained ResNet50 without top layer
base_model = ResNet50(
    include_top=False,
    weights='imagenet',
    input_shape=(IMG_DIMS, IMG_DIMS, 3)
)

# Freeze base model layers
for layer in base_model.layers:
    layer.trainable = False

# Build custom top layers for 5-class classification
x = base_model(input_tensor)  # Attach input tensor to the base model
x = Flatten()(x)
x = BatchNormalization()(x)
x = Dense(1024, activation='relu')(x)
x = Dropout(0.4)(x)
x = BatchNormalization()(x)
x = Dense(512, activation='relu')(x)
x = Dropout(0.4)(x)
x = BatchNormalization()(x)
x = Dense(256, activation='relu')(x)
output = Dense(5, activation='softmax')(x)  # 5 output classes

# Create the final model
model = Model(inputs=input_tensor, outputs=output)

# Compile the model
model.compile(
    optimizer=Adam(learning_rate=LEARNING_RATE),
    loss='categorical_crossentropy',
    metrics=METRICS
)

# ================================
# 5. Handle Class Imbalance
# ================================
# Compute class weights for balancing
cls_wt = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(train_data.classes),
    y=train_data.classes
)
class_weight = {i: cls_wt[i] for i in range(len(cls_wt))}
print("Class Weights: ", class_weight)

# ================================
# 6. Learning Rate Scheduler
# ================================
def scheduler(epoch):
    if epoch > 10:
        return LEARNING_RATE * 0.1
    return LEARNING_RATE

callback_lr = LearningRateScheduler(scheduler)

# ================================
# 7. Early Stopping
# ================================
callback_es = EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True
)

# ================================
# 8. Train the Model
# ================================
history = model.fit(
    train_data,
    epochs=EPOCHS,
    validation_data=val_data,
    class_weight=class_weight,
    callbacks=[callback_lr, callback_es]
)

# ================================
# 9. Evaluate the Model
# ================================
test_loss, test_accuracy, test_precision, test_recall = model.evaluate(test_data)
print(f"Test Accuracy: {test_accuracy:.4f}")
print(f"Test Precision: {test_precision:.4f}")
print(f"Test Recall: {test_recall:.4f}")

# ================================
# 10. Classification Report
# ================================
y_true = test_data.classes
y_pred = model.predict(test_data)
y_pred_classes = np.argmax(y_pred, axis=1)

# Generate classification report
report = classification_report(y_true, y_pred_classes, target_names=test_data.class_indices.keys())
print(report)

# ================================
# 11. Confusion Matrix
# ================================
cm = confusion_matrix(y_true, y_pred_classes)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=test_data.class_indices.keys())
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.show()

# ================================
# 12. Plot Training History
# ================================
plt.figure(figsize=(12, 5))
plt.plot(history.history['categorical_accuracy'], label='Training Accuracy')
plt.plot(history.history['val_categorical_accuracy'], label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

plt.figure(figsize=(12, 5))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

from google.colab import drive
drive.mount('/content/drive')